
<html>
<head></head>
<body>
<h1>flash-attn: Python wheels for CUDA cu123 + torch2.4 + cxx11abiFALSE</h1>
<a href="https://github.com/Dao-AILab/flash-attention/releases/download/v2.6.1/flash_attn-2.6.1%2Bcu123torch2.4cxx11abiFALSE-cp310-cp310-linux_x86_64.whl">flash_attn-2.6.1+cu123torch2.4cxx11abiFALSE-cp310-cp310-linux_x86_64.whl</a><br>
<a href="https://github.com/Dao-AILab/flash-attention/releases/download/v2.6.1/flash_attn-2.6.1%2Bcu123torch2.4cxx11abiFALSE-cp311-cp311-linux_x86_64.whl">flash_attn-2.6.1+cu123torch2.4cxx11abiFALSE-cp311-cp311-linux_x86_64.whl</a><br>
<a href="https://github.com/Dao-AILab/flash-attention/releases/download/v2.6.1/flash_attn-2.6.1%2Bcu123torch2.4cxx11abiFALSE-cp312-cp312-linux_x86_64.whl">flash_attn-2.6.1+cu123torch2.4cxx11abiFALSE-cp312-cp312-linux_x86_64.whl</a><br>
<a href="https://github.com/Dao-AILab/flash-attention/releases/download/v2.6.1/flash_attn-2.6.1%2Bcu123torch2.4cxx11abiFALSE-cp38-cp38-linux_x86_64.whl">flash_attn-2.6.1+cu123torch2.4cxx11abiFALSE-cp38-cp38-linux_x86_64.whl</a><br>
<a href="https://github.com/Dao-AILab/flash-attention/releases/download/v2.6.1/flash_attn-2.6.1%2Bcu123torch2.4cxx11abiFALSE-cp39-cp39-linux_x86_64.whl">flash_attn-2.6.1+cu123torch2.4cxx11abiFALSE-cp39-cp39-linux_x86_64.whl</a><br>
<a href="https://github.com/Dao-AILab/flash-attention/releases/download/v2.6.2/flash_attn-2.6.2%2Bcu123torch2.4cxx11abiFALSE-cp310-cp310-linux_x86_64.whl">flash_attn-2.6.2+cu123torch2.4cxx11abiFALSE-cp310-cp310-linux_x86_64.whl</a><br>
<a href="https://github.com/Dao-AILab/flash-attention/releases/download/v2.6.2/flash_attn-2.6.2%2Bcu123torch2.4cxx11abiFALSE-cp311-cp311-linux_x86_64.whl">flash_attn-2.6.2+cu123torch2.4cxx11abiFALSE-cp311-cp311-linux_x86_64.whl</a><br>
<a href="https://github.com/Dao-AILab/flash-attention/releases/download/v2.6.2/flash_attn-2.6.2%2Bcu123torch2.4cxx11abiFALSE-cp312-cp312-linux_x86_64.whl">flash_attn-2.6.2+cu123torch2.4cxx11abiFALSE-cp312-cp312-linux_x86_64.whl</a><br>
<a href="https://github.com/Dao-AILab/flash-attention/releases/download/v2.6.2/flash_attn-2.6.2%2Bcu123torch2.4cxx11abiFALSE-cp38-cp38-linux_x86_64.whl">flash_attn-2.6.2+cu123torch2.4cxx11abiFALSE-cp38-cp38-linux_x86_64.whl</a><br>
<a href="https://github.com/Dao-AILab/flash-attention/releases/download/v2.6.2/flash_attn-2.6.2%2Bcu123torch2.4cxx11abiFALSE-cp39-cp39-linux_x86_64.whl">flash_attn-2.6.2+cu123torch2.4cxx11abiFALSE-cp39-cp39-linux_x86_64.whl</a><br>
<a href="https://github.com/Dao-AILab/flash-attention/releases/download/v2.6.3/flash_attn-2.6.3%2Bcu123torch2.4cxx11abiFALSE-cp310-cp310-linux_x86_64.whl">flash_attn-2.6.3+cu123torch2.4cxx11abiFALSE-cp310-cp310-linux_x86_64.whl</a><br>
<a href="https://github.com/Dao-AILab/flash-attention/releases/download/v2.6.3/flash_attn-2.6.3%2Bcu123torch2.4cxx11abiFALSE-cp311-cp311-linux_x86_64.whl">flash_attn-2.6.3+cu123torch2.4cxx11abiFALSE-cp311-cp311-linux_x86_64.whl</a><br>
<a href="https://github.com/Dao-AILab/flash-attention/releases/download/v2.6.3/flash_attn-2.6.3%2Bcu123torch2.4cxx11abiFALSE-cp312-cp312-linux_x86_64.whl">flash_attn-2.6.3+cu123torch2.4cxx11abiFALSE-cp312-cp312-linux_x86_64.whl</a><br>
<a href="https://github.com/Dao-AILab/flash-attention/releases/download/v2.6.3/flash_attn-2.6.3%2Bcu123torch2.4cxx11abiFALSE-cp38-cp38-linux_x86_64.whl">flash_attn-2.6.3+cu123torch2.4cxx11abiFALSE-cp38-cp38-linux_x86_64.whl</a><br>
<a href="https://github.com/Dao-AILab/flash-attention/releases/download/v2.6.3/flash_attn-2.6.3%2Bcu123torch2.4cxx11abiFALSE-cp39-cp39-linux_x86_64.whl">flash_attn-2.6.3+cu123torch2.4cxx11abiFALSE-cp39-cp39-linux_x86_64.whl</a><br>
</body>
</html>
