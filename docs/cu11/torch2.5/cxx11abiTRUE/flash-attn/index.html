
<html>
<head></head>
<body>
<h1>flash-attn: Python wheels for CUDA cu11 + torch2.5 + cxx11abiTRUE</h1>
<a href="https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.3/flash_attn-2.7.3%2Bcu11torch2.5cxx11abiTRUE-cp39-cp39-linux_x86_64.whl">flash_attn-2.7.3+cu11torch2.5cxx11abiTRUE-cp39-cp39-linux_x86_64.whl</a><br>
<a href="https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.3/flash_attn-2.7.3%2Bcu11torch2.5cxx11abiTRUE-cp313-cp313-linux_x86_64.whl">flash_attn-2.7.3+cu11torch2.5cxx11abiTRUE-cp313-cp313-linux_x86_64.whl</a><br>
<a href="https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.3/flash_attn-2.7.3%2Bcu11torch2.5cxx11abiTRUE-cp312-cp312-linux_x86_64.whl">flash_attn-2.7.3+cu11torch2.5cxx11abiTRUE-cp312-cp312-linux_x86_64.whl</a><br>
<a href="https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.3/flash_attn-2.7.3%2Bcu11torch2.5cxx11abiTRUE-cp311-cp311-linux_x86_64.whl">flash_attn-2.7.3+cu11torch2.5cxx11abiTRUE-cp311-cp311-linux_x86_64.whl</a><br>
<a href="https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.3/flash_attn-2.7.3%2Bcu11torch2.5cxx11abiTRUE-cp310-cp310-linux_x86_64.whl">flash_attn-2.7.3+cu11torch2.5cxx11abiTRUE-cp310-cp310-linux_x86_64.whl</a><br>
<a href="https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.2.post1/flash_attn-2.7.2.post1%2Bcu11torch2.5cxx11abiTRUE-cp39-cp39-linux_x86_64.whl">flash_attn-2.7.2.post1+cu11torch2.5cxx11abiTRUE-cp39-cp39-linux_x86_64.whl</a><br>
<a href="https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.2.post1/flash_attn-2.7.2.post1%2Bcu11torch2.5cxx11abiTRUE-cp313-cp313-linux_x86_64.whl">flash_attn-2.7.2.post1+cu11torch2.5cxx11abiTRUE-cp313-cp313-linux_x86_64.whl</a><br>
<a href="https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.2.post1/flash_attn-2.7.2.post1%2Bcu11torch2.5cxx11abiTRUE-cp312-cp312-linux_x86_64.whl">flash_attn-2.7.2.post1+cu11torch2.5cxx11abiTRUE-cp312-cp312-linux_x86_64.whl</a><br>
<a href="https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.2.post1/flash_attn-2.7.2.post1%2Bcu11torch2.5cxx11abiTRUE-cp311-cp311-linux_x86_64.whl">flash_attn-2.7.2.post1+cu11torch2.5cxx11abiTRUE-cp311-cp311-linux_x86_64.whl</a><br>
<a href="https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.2.post1/flash_attn-2.7.2.post1%2Bcu11torch2.5cxx11abiTRUE-cp310-cp310-linux_x86_64.whl">flash_attn-2.7.2.post1+cu11torch2.5cxx11abiTRUE-cp310-cp310-linux_x86_64.whl</a><br>
<a href="https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.1.post4/flash_attn-2.7.1.post4%2Bcu11torch2.5cxx11abiTRUE-cp39-cp39-linux_x86_64.whl">flash_attn-2.7.1.post4+cu11torch2.5cxx11abiTRUE-cp39-cp39-linux_x86_64.whl</a><br>
<a href="https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.1.post4/flash_attn-2.7.1.post4%2Bcu11torch2.5cxx11abiTRUE-cp313-cp313-linux_x86_64.whl">flash_attn-2.7.1.post4+cu11torch2.5cxx11abiTRUE-cp313-cp313-linux_x86_64.whl</a><br>
<a href="https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.1.post4/flash_attn-2.7.1.post4%2Bcu11torch2.5cxx11abiTRUE-cp312-cp312-linux_x86_64.whl">flash_attn-2.7.1.post4+cu11torch2.5cxx11abiTRUE-cp312-cp312-linux_x86_64.whl</a><br>
<a href="https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.1.post4/flash_attn-2.7.1.post4%2Bcu11torch2.5cxx11abiTRUE-cp311-cp311-linux_x86_64.whl">flash_attn-2.7.1.post4+cu11torch2.5cxx11abiTRUE-cp311-cp311-linux_x86_64.whl</a><br>
<a href="https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.1.post4/flash_attn-2.7.1.post4%2Bcu11torch2.5cxx11abiTRUE-cp310-cp310-linux_x86_64.whl">flash_attn-2.7.1.post4+cu11torch2.5cxx11abiTRUE-cp310-cp310-linux_x86_64.whl</a><br>
<a href="https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.0.post2/flash_attn-2.7.0.post2%2Bcu11torch2.5cxx11abiTRUE-cp39-cp39-linux_x86_64.whl">flash_attn-2.7.0.post2+cu11torch2.5cxx11abiTRUE-cp39-cp39-linux_x86_64.whl</a><br>
<a href="https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.0.post2/flash_attn-2.7.0.post2%2Bcu11torch2.5cxx11abiTRUE-cp312-cp312-linux_x86_64.whl">flash_attn-2.7.0.post2+cu11torch2.5cxx11abiTRUE-cp312-cp312-linux_x86_64.whl</a><br>
<a href="https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.0.post2/flash_attn-2.7.0.post2%2Bcu11torch2.5cxx11abiTRUE-cp311-cp311-linux_x86_64.whl">flash_attn-2.7.0.post2+cu11torch2.5cxx11abiTRUE-cp311-cp311-linux_x86_64.whl</a><br>
<a href="https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.0.post2/flash_attn-2.7.0.post2%2Bcu11torch2.5cxx11abiTRUE-cp310-cp310-linux_x86_64.whl">flash_attn-2.7.0.post2+cu11torch2.5cxx11abiTRUE-cp310-cp310-linux_x86_64.whl</a><br>
<a href="https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.0.post1/flash_attn-2.7.0.post1%2Bcu11torch2.5cxx11abiTRUE-cp312-cp312-linux_x86_64.whl">flash_attn-2.7.0.post1+cu11torch2.5cxx11abiTRUE-cp312-cp312-linux_x86_64.whl</a><br>
<a href="https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.0.post1/flash_attn-2.7.0.post1%2Bcu11torch2.5cxx11abiTRUE-cp311-cp311-linux_x86_64.whl">flash_attn-2.7.0.post1+cu11torch2.5cxx11abiTRUE-cp311-cp311-linux_x86_64.whl</a><br>
<a href="https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.0.post1/flash_attn-2.7.0.post1%2Bcu11torch2.5cxx11abiTRUE-cp310-cp310-linux_x86_64.whl">flash_attn-2.7.0.post1+cu11torch2.5cxx11abiTRUE-cp310-cp310-linux_x86_64.whl</a><br>
</body>
</html>
